# Multiverse analysis {#sec-Cap.3}

## Introduzione

Nel seguente capitolo si introduce il concetto di *multiverse* e si presentano i diversi approcci esplorativi e inferenziali di analisi del *multiverse*. Infine, vengono discussi i principali limiti di questi approcci e le prospettive future per l'utilizzo di questi metodi nella ricerca in ambito psicologico.

## Presupposti e obiettivi degli approcci multiverse

Una delle principali cause della crisi di replicabilità in ambito psicologico riguarda i gradi di libertà dei ricercatori nelle diverse fasi di svolgimento di uno studio, cioè l'arbitrarietà con cui si selezionano le procedure di raccolta, codifica e analisi dei dati [@simmons2011].

I risultati presenti in letteratura, infatti, spesso non sono solo il frutto di vere e proprie *Questionable Research Practices* [QRPs, @john2012], ma sono più semplicemente il prodotto di una serie di scelte arbitrarie compiute dai ricercatori in diversi momenti della ricerca [@steegen2016; @gelman2013]. Il ricercatore, nella pratica, è costretto a compiere una serie di decisioni relative agli strumenti di misurazione da utilizzare, ai criteri di esclusione/inclusione di determinati valori o di unità statistiche, ai modelli statistici da utilizzare per analizzare i dati rilevati, ecc. [@steegen2016; @patel2015; @simonsohn; @girardi2024]. Molte volte tali decisioni sono semplici ed immediate, in quanto le evidenze e la teoria presenti in letteratura indicano chiaramente la superiorità di un'alternativa rispetto ad un'altra [@delgiudice2021]. Altre volte, però, il ricercatore si ritrova a dover prendere tali decisioni in modo arbitrario, senza poter sapere in anticipo quale alternativa sia la più opportuna [@steegen2016; @delgiudice2021]. L'arbitrarietà decisionale del ricercatore in questi numerosi "punti di svolta" durante lo svolgimento di uno studio genera una molteplicità di risultati e conclusioni alternativi, in qualche modo ugualmente corretti e legittimi [@gelman2013; @steegen2016; @simonsohn; @girardi2024; @delgiudice2021]. L'insieme di tutte queste procedure analitiche alternative rappresenta il cosiddetto *multiverse*.

Il problema emerso dalla recente crisi di credibilità è che i ricercatori tendono ad analizzare tutte queste possibili alternative, riportando però solamente una delle "vie" decisionali intraprese [@steegen2016; @delgiudice2021] - tra le tante del cosiddetto *garden of forking paths* [@gelman2013] - come rappresentato in @fig-garden. Solitamente, poi, i risultati riportati sono solo quelli più "convenienti", cioè quelli che hanno riportato un risultato significativo e in linea con le ipotesi iniziali del ricercatore [@simonsohn].

Inoltre, proprio a causa dei numerosi test che vengono condotti sui dati raccolti (a loro volta frutto di procedure arbitrarie di raccolta e codifica dei dati), la probabilità di ottenere dei falsi positivi aumenta notevolmente [@gelman2013; @götz2024]. Infatti, effettuare diversi test senza tener conto di questi confronti multipli, aumenta la probabilità di commettere un errore del I tipo al di sopra del livello nominale scelto [ad es., alpha = 5%, @götz2024]. Ciò rappresenta una delle principali cause della crisi di replicabilità nell'ambito della ricerca psicologica [@head2015; @bakker2012; @errington2021; @scheel2022].

Tutti questi elementi contribuiscono a ridurre la trasparenza e la replicabilità degli studi psicologici [@steegen2016; @simonsohn; @girardi2024]. Di conseguenza, per far fronte a questo problema, negli ultimi anni sono stati proposti diversi metodi per analizzare e riportare tutte le possibili combinazioni di procedure per la codifica e l'analisi dei dati che compongono ciascun *multiverse* [@steegen2016; @patel2015; @simonsohn; @girardi2024]. Infatti, attraverso l'analisi del *multiverse,* e la conseguente presentazione di tutti i risultati ottenuti, è possibile comprendere: quanto l'effetto indagato sia robusto, quali decisioni metodologiche determinino la sua eventuale fragilità e se l'effetto sia effettivamente presente o se sia solamente il frutto delle scelte arbitrarie del ricercatore [@patel2015; @steegen2016; @simonsohn; @girardi2024].

![*Garden of forking paths*: differenza tra il procedimento "tradizionale" di analisi e presentazione dei risultati (immagine a., sinistra) e i metodi *multiverse* [immagine c., destra, @dragicevic2019, p. 2].](images/Garden%20merged.png){#fig-garden fig-align="center" width="396"}

I "metodi *multiverse"* si suddividono principalmente in approcci di tipo esplorativo, come la *Multiverse Analysis* [MA, @steegen2016] o la *Vibration of Effects* [VoE, @patel2015], e in approcci inferenziali, come la *Specification Curve* *Analysis* [SCA, @simonsohn] o la *Post-selection Inference in Multiverse Analysis* [PIMA, @girardi2024] e verranno approfonditi nel seguente paragrafo.

## Applicazioni dei metodi multiverse

Come già affermato, i metodi *multiverse* possono essere di tipo esplorativo o inferenziale. I primi, come la MA e la VoE, si limitano a presentare in modo descrittivo i risultati derivanti da ciascuno scenario che forma il *multiverse* [@patel2015; @steegen2016]; i secondi, invece, come la SCA o la PIMA, offrono la possibilità di compiere delle inferenze statistiche sull'eventuale presenza e significatività dell'effetto studiato all'interno del *multiverse* [@girardi2024; @simonsohn].

Di seguito saranno presentate le principali caratteristiche di ciascun approccio.

### Approcci esplorativi

Uno dei primi metodi *multiverse* apparsi in letteratura è la *Vibration of Effects* [VoE, @patel2015]. L'obiettivo della VoE è "descrivere la misura con cui un'associazione stimata cambi a seconda dei molteplici approcci analitici utilizzati" [@patel2015, p. 2].

Questo approccio *multiverse* affronta il problema specifico dell'arbitrarietà nella selezione e specificazione delle variabili da includere/escludere all'interno di modelli statistici multivariati [@patel2015]. Nella letteratura medica-epidemiologica (come in quella psicologica), infatti, quasi tutti gli studi riportano i risultati ottenuti da una singola specificazione del modello statistico utilizzato per analizzare i dati [@patel2015]. Inoltre, i ricercatori spesso riportano i risultati derivanti da uno solo dei numerosi modelli statistici tra quelli utilizzati per l'analisi dei dati; di conseguenza, è più probabile che l'eventuale effetto riportato rappresenti un falso positivo [@götz2024].

Attraverso la VoE, comunque, è possibile intuire quanto l'inclusione o l'esclusione di determinate variabili all'interno di un modello multivariato (e le loro reciproche associazioni) influiscano sulla dimensione e sulla significatività dell'effetto indagato [@patel2015]. Nella pratica, la VoE consiste nello stimare la distribuzione (denominata dagli autori "vibrazione") degli effetti - e dei relativi *p-value -* di tutte le possibili specificazioni del modello statistico utilizzato [@patel2015]. Come illustrato in @fig-VoE, tanto più ampia è la distribuzione degli effetti di tutte le possibili specificazioni (e tanto più bassa è la proporzione di *p-value* significativi sul totale), tanto più l'effetto indagato è fragile e influenzato dal modello statistico selezionato [@patel2015],

![Rappresentazione grafica della VoE: ciascun punto corrisponde all'effect size e al corrispettivo p-value di ogni singola specificazione del modello statistico utilizzato per analizzare i dati \[\@delgiudice2021, p. 9\].](images/VoE.png){#fig-VoE fig-align="center" width="281"}

Un altro approccio sviluppato per far fronte al problema della scarsa trasparenza e dell'arbitrarietà nella presentazione dei risultati nell'ambito della ricerca psicologica è la *Multiverse Analysis* [MA, @steegen2016]. La MA consiste nel riportare i risultati delle analisi condotte su tutti i possibili e legittimi dataset costruiti a partire dai dati grezzi raccolti [@steegen2016].

Quando si effettua uno studio, solitamente, i dati grezzi raccolti dalle unità statistiche vengono codificati in un unico dataset, sul quale poi vengono svolte le analisi statistiche [@steegen2016]. Le scelte che portano alla costruzione di uno specifico dataset (ad es., l'inclusione o l'esclusione degli *outlier*, il metodo di gestione dei dati mancanti, le modalità di discretizzazione o di dicotomizzazione della variabili, ecc.), però, sono spesso arbitrarie e non supportate da evidenze o dalla teoria presenti in letteratura [@steegen2016]. Da una serie di dati grezzi raccolti, quindi, è possibile costruire numerosi dataset ugualmente legittimi [il cosidetto *multiverse* dei dataset, @steegen2016].

Nella pratica, dunque, @steegen2016 propongono di generare tutti i ragionevoli dataset a partire dai dati grezzi raccolti, per poi eseguire la stessa analisi statistica su ciascun dataset ed ottenere i singoli effetti e corrispettivi *p-value*. È importante sottolineare che gli autori hanno ben specificato come l'analisi debba essere condotta non su tutte le possibili combinazioni di scelte di codifica, ma solo su quelle ragionevoli, cioè supportate dalle evidenze o dalla teoria.

I risultati della MA possono poi essere rappresentati graficamente sottoforma di distribuzioni di frequenza dei *p-value* (a sinistra nella @fig-MA) oppure attraverso delle griglie di *p-value* che evidenziano le combinazioni di scelta di codifica dei dati (a destra nella @fig-MA). Quest'ultima modalità, inoltre, permette di visualizzare quali decisioni di codifica esercitano un maggior impatto sulla significatività dei risultati [@steegen2016].

![Rappresentazione grafica dei risultati di una Multiverse Analysis: a sinistra, la distribuzione di frequenza dei p-value risultanti dalle analisi condotte su ciasun dataset del multiverse; a destra, la griglia delle possibili combinazioni delle scelte di codifica dei dati con rispettivi p-value (in grigio evidenziate le combinazioni di scelte con valori \$p \\leq .05\$); le sigle (ad es., F1, NMO2, ecc.) rappresentano gli acronimi delle diverse possibili scelte di processamento dei dati \[ad es., NMO](images/MA%20DefDef.png){#fig-MA fig-align="center" width="600"}

In sintesi, quindi, gli obiettivi della MA sono promuovere la trasparenza degli studi presenti in letteratura e presentare un metodo per comprendere quanto i risultati di uno studio siano influenzati dai gradi di libertà dei ricercatori durante la fase di codifica dei dati [@steegen2016].

Ciononostante, la MA e la VoE, data la loro natura descrittiva, non consentono di valutare se all'interno del *multiverse* indagato sia effettivamente presente un effetto, cioè che i risultati significativi ottenuti non siano in realtà dei falsi positivi [@girardi2024].

### Approcci inferenziali

Come già discusso, il principale limite dei metodi esplorativi descritti sopra consiste nella loro natura descrittiva [@girardi2024]. Attraverso la VoE e la *Multiverse Analysis*, infatti, non è possibile trarre conclusioni circa la reale significatività dell'effetto indagato dall'intero *multiverse*. Per questo motivo sono stati sviluppati dei metodi inferenziali che, al contrario, consentono di effettuare inferenze sul *multiverse* dei risultati [@simonsohn; @girardi2024]. Uno di questi metodi, ad esempio, consiste nel calcolo di un *p-value* globale, sul quale viene poi effettuato un test la cui ipotesi nulla prevede che tutti gli scenari del *multiverse* abbiano un effetto uguale a zero, mentre l'ipotesi alternativa prevede che almeno uno degli scenari inclusi nel *multiverse* presenti un effetto diverso da zero [@girardi2024]. Gli approcci *multiverse* di tipo inferenziale presenti ad oggi in letteratura sono la *Specification Curve Analysis* [SCA, @simonsohn] e la *Post-selection Inference in Multiverse Analysis* [PIMA, @girardi2024].

La *Specification Curve Analysis* (SCA) "consiste nel riportare i risultati di tutte le specificazioni 'ragionevoli' \[incluse nel *multiverse*\], cioè che siano coerenti con la domanda di ricerca, statisticamente valide e non ripetitive rispetto ad altre specificazioni" [@simonsohn, p. 2]. Con "specificazioni" si intendono tutte le combinazioni di scelte del ricercatore relative alle decisioni analitiche, come per esempio i criteri per la codifica dei dati o i modelli statistici utilizzati per l'analisi. L'obiettivo della SCA è quindi di favorire la trasparenza nella comunicazione di tutti i possibili risultati (e non solo della porzione più conveniente per il ricercatore), riducendo l'impatto delle decisioni arbitrarie sui risultati finali [@simonsohn].

La SCA integra l'approccio descrittivo discusso precedentemente con quello inferenziale [@simonsohn]. Nella pratica, una volta generato il *multiverse* di tutte le possibili specificazioni ragionevoli, attraverso la SCA è possibile presentare graficamente e in modo descrittivo la distribuzione dei risultati [@simonsohn]. Questo primo passaggio permette già di individuare quali combinazioni di scelte influiscono maggiormente sulla significatività dei risultati, come è possibile vedere nella [@fig-SCA1, @simonsohn].

![Esempio di *Specification Curve* descrittiva: nel pannello sopra, la distribuzione degli effetti osservati relativi ad ogni specificazione con in nero gli effetti con p \< .05; nel pannello sotto, le combinazioni di decisioni analitiche relative ad ogni specificazione [@simonsohn2020].](images/SC1.png){#fig-SCA1 fig-align="center" width="488"}

La SCA, inoltre, permette di effettuare un test inferenziale globale sull'intero *multiverse* di specificazioni [@simonsohn]. Nello specifico, @simonsohn propongono tre diverse statistiche test per effettuare inferenze statistiche a partire dalla SCA. La prima consiste nel testare se la mediana dell'effetto stimato di tutte le specificazioni ragionevoli sia uguale o più estrema dell'effetto mediano che ci si aspetterebbe qualora l'effetto reale fosse zero. La seconda si basa invece sulla proporzione di specificazioni statisticamente significative sul totale (ad es., con p \< .05 e nella direzione prevista) e valuta se questa proporzione sia uguale o più estrema di quella che ci si aspetterebbe qualora l'effetto reale fosse zero. Infine, la terza statistica test consiste nell'aggregare e fare una media di tutti i valori Z associati ai *p-value* delle specificazioni (ad es., Z = 1.96 per p = .05); poi si valuta se questo valore medio dei punti Z sia uguale o più estremo rispetto al valore che ci si aspetterebbe qualora l'effetto reale fosse zero [@simonsohn].

Per eseguire queste statistiche test, @simonsohn propongono di generare le distribuzioni degli effetti delle specificazioni nulle (cioè in cui l'effetto reale sottostante è zero e per le quali, quindi, è certo che l'ipotesi nulla sia vera) attraverso un metodo di ricampionamento; le distribuzioni degli effetti delle specificazioni nulle sono poi estratte casualmente e confrontate con i valori stimati della *Specification Curve* effettivamente osservata.

È poi possibile riportare graficamente i risultati delle statistiche test: ad esempio rappresentando sia gli effetti stimati della *Specification Curve* osservata sia la mediana delle distribuzioni nulle (e i rispettivi 2.5esimo e 97.5esimo percentile), come in @fig-SCA2.

![Esempio di rappresentazione grafica della procedura inferenziale della SCA: la linea blu tratteggiata riporta gli effetti osservati della *Specification Curve* effettiva; la linea nera tratteggiata è la mediana delle distribuzioni degli effetti delle specificazioni nulle, con in grigio il 2.5esimo e il 97.5esimo percentile [@simonsohn2020].](images/Sca2.png){#fig-SCA2 fig-align="center" width="407"}

Grazie alle SCA, dunque, è possibile comprendere quanto i risultati riportati siano significativi e quali decisioni arbitrarie dei ricercatori nelle diverse fasi di analisi dei dati influiscano su tale significatività [@simonsohn].

La SCA presenta però anche degli importanti limiti. Innanzitutto, è applicabile solo ai modelli lineari, il che ne restringe notevolmente l'utilizzo [@girardi2024]. Inoltre, la SCA consente di testare solo una singola ipotesi alla volta e non permette di selezionare le specificazioni significative all'interno della *Specification Curve*. Questo metodo, infatti, offre solo un *weak FWER control* (cioè un test globale del *p-value*), non consentendo dunque di controllare l'inflazione dell'errore del I Tipo al livello dei singoli *p-value* ottenuti [*strong FWER controL*, @girardi2024].

Per questi motivi, è stato recentemente proposto un altro approccio inferenziale all'analisi dei *multiverse* che supera tali limiti: la *Post-selection Inference in Multiverse Analysis* [PIMA, @girardi2024]. La PIMA è un "approccio inferenziale generale che prende in considerazione tutte le possibili specificazioni, a partire dalla fase di codifica dei dati e fino ai possibili modelli statistici \[utilizzati per analizzare i dati\]" [@girardi2024, p. 542]. Inoltre, è applicabile a tutti i Modelli Lineari Generalizzati (GLMs) e consente di testare se un determinato predittore sia effettivamente associato alla variabile dipendente offrendo sia un test globale del *p-value* (*weak FWER control*) che un test per i singoli *p-value* [*strong FWER control,* @girardi2024].

Con il metodo della PIMA, dunque, i ricercatori possono analizzare i risultati di tutte le possibili specificazioni che compongono il *multiverse*, testare la loro significatività e selezionare solo le analisi statisticamente significative [@girardi2024]. Nella pratica, quindi, la PIMA consente di effettuare una sorta di *p-hacking* legittimo: grazie a questo metodo, infatti, è possibile effettuare delle "inferenze selettive" (*selective inferences*) in modo trasparente e mantenendo la probabilità di ottenere dei falsi positivi al di sotto della soglia tipica [ad es., $\alpha$ = .05, @girardi2024].

Nello specifico, la PIMA permette di condurre inferenze sull'intero *multiverse* attraverso una procedura di ricampionamento e di mantenere sotto controllo l'errore del I Tipo grazie ad un metodo di aggiustamento per i confronti multipli dei singoli *p-value* [metodo *maxT*, per una trattazione più approfondita, vedi @girardi2024]. È poi possibile rappresentare graficamente i risultati ottenuti evidenziando le combinazioni di scelte e la significatività dei singoli effetti prima e dopo l'aggiustamento con il metodo *maxT*, come in @fig-PIMA.

![Esempio di distribuzione degli effetti e relativi *p-value* di ogni specificazione del *multiverse* prima e dopo l'aggiustamento tramite il metodo *maxT*. In rosso le specificazioni che non sono mai risultate statisticamente significative; in verde quelle significative solo prima dell'aggiustamento di controllo dell'errore del I Tipo e in azzurro le specificazioni significative anche dopo la correzione.](images/PIMA.png){#fig-PIMA fig-align="center" width="355"}

In sintesi, dunque, la PIMA consente: di effettuare un test generale per controllare se almeno una delle specificazioni che compongono il *multiverse* rifiuti o meno l'ipotesi nulla di assenza dell'effetto (*weak FWER control*), di selezionare le specificazioni statisticamente significative (cioè con un *adjusted p-value* \< .05) mantenendo un errore del I Tipo al di sotto del valore soglia (*strong FWER control*) e di riportare la proporzione minima di specificazioni con un effetto statisticamente significativo rispetto al totale di quelle che compongono il *multiverse* [@girardi2024].

In conclusione, la PIMA rappresenta un importante sviluppo all'interno dei metodi *multiverse*, poiché permette di superare la natura puramente descrittiva degli approcci esplorativi (come la VoE di @patel2015 o la *Multiverse Analysis* di @steegen2016) e, inoltre, offre una procedura inferenziale applicabile a tutti i GLMs, attraverso la quale è possibile selezionare le singole specificazioni significative mantenendo sotto controllo l'errore del I Tipo [@girardi2024].

## Conclusioni: limiti e prospettive degli approcci multiverse

Come discusso in questo capitolo e nel @sec-Cap.1, il *p-hacking* e i gradi di libertà dei ricercatori rappresentano le cause principali della crisi di replicabilità nell'ambito della ricerca psicologica [@head2015; @bakker2012; @errington2021; @scheel2022]. Il *p-hacking* è frutto del *selective reporting*, che consiste nell'effettuare molteplici test statistici sui dati rilevati per poi riportare solo quelli che hanno prodotto risultati significativi e in linea con le ipotesi iniziali [@head2015]. Il rischio principale connesso a tale pratica - oltre alla scarsa trasparenza - riguarda l'inflazione dell'errore del I Tipo, che si verifica quando non si tiene conto del problema dei confronti multipli [@götz2024]. Poi, come già spiegato, i gradi di libertà dei ricercatori implicano che molte volte i risultati e le conclusioni presenti in letteratura rappresentino solo una piccola porzione di tutti i possibili risultati che si sarebbero potuti ottenere effettuando altre scelte analitiche di codifica e analisi dei dati [@simmons2011; @steegen2016; @gelman2013; @simonsohn; @girardi2024].

Il *p-hacking* e i gradi di libertà dei ricercatori, dunque, fanno sì che molti studi in letteratura riportino risultati e conclusioni distorti, poco rappresentativi o addirittura fuorvianti [@head2015; @simmons2011; @steegen2016]. Per risolvere questo problema sono state proposte diverse soluzioni, come la pre-registrazione degli studi e i *Registered Reports* [RRs, @nosek2018; @lakens2019; @nosek2014; @scheel2021]. La pre-registrazione e i RRs rappresentano un ottimo strumento per ridurre l'impatto del *selective reporting*, però al tempo stesso risultano a volte troppo rigidi, poiché non sempre è possibile sapere in anticipo quali analisi sarà necessario effettuare sui dati raccolti; inoltre, non garantiscono che le specificazioni dei modelli statistici scelti a priori siano rappresentative e a loro volta non arbitrarie [@delgiudice2021].

Gli approcci *multiverse*, al contrario, rappresentano un'alternativa efficace per risolvere il problema del *selective reporting* e della scarsa trasparenza dovuta ai gradi di libertà dei ricercatori [@steegen2016; @simonsohn; @girardi2024; @delgiudice2021]. Gli approcci *multiverse*, infatti, favoriscono la trasparenza perché riportano tutte le analisi svolte sui dati raccolti [@steegen2016; @simonsohn; @girardi2024]. Ciò permette di comprendere se e quali scelte analitiche arbitrarie effettuate dai ricercatori durante ciascuna fase dello studio influiscono sui risultati finali [@steegen2016; @simonsohn; @girardi2024]. Gli approcci *multiverse*, inoltre, consentono di valutare la robustezza e la stabilità dei risultati all'interno del *multiverse*, cioè quanto l'effetto indagato sia solido o sia più probabilmente un artificio frutto dei gradi di libertà dei ricercatori [@steegen2016; @simonsohn; @girardi2024].

Anche gli approcci *multiverse*, però, presentano alcuni limiti. Gli approcci *multiverse* esplorativi, come la VoE [@patel2015] o la *Multiverse Analysis* [@steegen2016], non permettono ad esempio di trarre conclusioni sulla reale presenza dell'effetto indagato all'interno del *multiverse* [@girardi2024]. Questi approcci, inoltre, sono accompagnati dal rischio di indurre il lettore (e spesso il ricercatore stesso) a effettuare inferenze a partire dalle analisi descrittive riportate [@girardi2024]; ciò potrebbe configurarsi come una diversa forma di *selective reporting*, poiché si tenderebbe a dare maggiore peso ai risultati significativi, ignorando se siano dei falsi positivi o addirittura non dando importanza a tutte le specificazioni con effetti nulli [@girardi2024].

Anche gli approcci inferenziali rappresentano solo delle soluzioni parziali al problema del *selective reporting* e dei gradi di libertà dei ricercatori [@simonsohn; @delgiudice2021]. La SCA [@simonsohn], ad esempio, permette di effettuare inferenze solo su modelli lineari; inoltre, non consente di testare più di un'ipotesi e offre solo un *weak FWER control* del *p-value* globale all'interno del *multiverse* [@girardi2024].

Dal punto di vista applicativo, però, il metodo della *Post-selection Inference in Multiverse Analysis* (PIMA) supera la maggior parte dei limiti degli approcci esplorativi e della SCA [@girardi2024]. Attraverso la PIMA, infatti, è possibile effettuare inferenze sull'intero *multiverse* includendo i Modelli Lineari Generalizzati (GLMs) e offrendo anche uno *strong FWER control* [@girardi2024]. Tutti questi fattori permettono dunque al ricercatore di selezionare le specificazioni significative in modo trasparente e, secondo determinate assunzioni, di mantenere sotto controllo il rischio di incorrere in falsi positivi [@girardi2024].

In generale, comunque, anche i diversi approcci *multiverse* prevedono delle scelte arbitrarie per la costruzione dei *multiverse* delle specificazioni [@steegen2016; @simonsohn]. Gli approcci *multiverse*, però, quantomeno discutono in modo trasparente le possibili scelte arbitrarie che possono influenzare i risultati e le conclusioni dello studio [@steegen2016; @simonsohn]. Ciononostante, spesso anche gli studi *multiverse* non contengono alcuna discussione relativa alle decisioni che hanno portato alla costruzione di uno specifico *multiverse* [@delgiudice2021].

Gli approcci *multiverse*, inoltre, sono spesso condotti su *multiverse* eccessivamente vasti [@delgiudice2021]. L'assenza in ambito psicologico di teorie solide e ben specificate e di processi metodologici uniformi, infatti, fa sì che frequentemente vengano incluse nel *multiverse* tutte le possibili combinazioni di scelte, e non solamente quelle frutto di decisioni arbitrarie [@delgiudice2021]. *Multiverse* così ampi possono poi essere percepiti come più esaustivi ed informativi, quando in realtà comportano comunque il rischio di riportare risultati ugualmente fuorvianti e distorti, "nascondendo effetti significativi all'interno di una massa di alternative insufficientemente giustificate" [@delgiudice2021, p.2]. Inoltre, *multiverse* eccessivamente ampi comportano criticità anche sul piano pratico: tanti più scenari sono inclusi nel *multiverse*, tanto più la correzione dei *p-value* per i confronti multipli è severa; ciò conseguentemente riduce la potenza statistica e, quindi, una maggiore probabilità di ottenere dei falsi negativi (errori del II Tipo).

Proprio per tali motivi, è dunque necessario sottolineare che, anche se gli approcci *multiverse* rappresentano un ottimo strumento per affrontare la crisi di replicabilità, il loro utilizzo richiede la presenza di basi teoriche e metodologiche solide e rigorose [@götz2024] - e di conseguenza il superamento della crisi teorica e di validità.

Proprio relativamente alla crisi teorica e di validità, gli approcci *multiverse* possono rappresentare un utile strumento per sviluppare i campi teorici e metodologici che nell'ambito della ricerca psicologica sono ancora poco maturi [@götz2024], come ad esempio si sta già iniziando a fare nelle aree di ricerca che utilizzano la tecnica della pupillometria [@calignano2024].

@delgiudice2021, poi, sottolineano anche come i *multiverse* dovrebbero comprendere solamente i risultati derivanti da scelte effettivamente arbitrarie, cioè per le quali "la teoria \[o l'evidenza\] non fornisce giustificazioni sufficienti per scegliere un'alternativa rispetto ad un'altra" [@delgiudice2021, p. 2]. A tal proposito, @delgiudice2021 propongono di riportare tutti i ragionamenti che hanno portato alla generazione di un determinato *multiverse*.

Per gli autori, l'analisi del *multiverse* dovrebbe essere effettuata unicamente sui *multiverse* composti da decisioni di Tipo E (Equivalenza di principio), cioè da scelte effettivamente arbitrarie per cui non è possibile decretare quale alternativa sia effettivamente la migliore. Una decisione di Tipo E, ad esempio, riguarda la scelta tra due strumenti di misura che valutano lo stesso costrutto con uguale validità e attendibilità. Al contrario, tutte le altre tipologie di decisioni, come quelle non arbitrarie (per cui cioè esistono delle opzioni effettivamente superiori alle altre) e quelle incerte (per le quali non ci sono sufficienti informazioni circa la superiorità di alcune alternative rispetto ad altre), non dovrebbero essere utilizzate per comporre un *multiverse*, o comunque dovrebbero essere utilizzate per effettuare analisi solamente di tipo esplorativo [@delgiudice2021].

Un'interessante prospettiva relativa agli approcci *multiverse* riguarda la proposta di @dragicevic2019 degli *Explorable Multiverse Analysis Reports* (EMARs). Gli EMARs consistono in articoli che il lettore può esplorare in modo interattivo, ad esempio modificando alcuni fattori delle analisi per vederne immediatamente i risultati [@dragicevic2019]. Ciò permetterebbe ai report dei *multiverse* di mantenere la complessità delle analisi, senza dover ridurre i risultati ai soli *p-value* e fornendo al lettore la possibilità di accedere alla totalità dei risultati [@dragicevic2019].

Ulteriormente, affinché i vantaggi degli approcci *multiverse* possano essere sfruttati a pieno è necessario che la trasparenza promossa da questi metodi sia sostenuta anche dagli *editor* delle riviste scientifiche, dagli enti che finanziano le ricerche e dai *referees* [@patel2015]. Queste componenti della comunità accademica, infatti, dovrebbero coordinarsi per far sì che i ricercatori riportino in modo trasparente e non distorto tutti i risultati delle analisi condotte sui dati, e non solo i risultati più convenienti [@patel2015].

Un altro promettente utilizzo del metodo *multiverse* riguarda le meta-analisi. I risultati stessi delle meta-analisi, infatti, sono spesso frutto di scelte arbitrarie da parte del ricercatore, relative ad esempio agli studi da includere/escludere, agli indici di *effect size* da utilizzare, alla scelta tra i modelli *random-effects* e *fixed effect*, ecc. Di conseguenza, il metodo *multiverse* è estremamente adatto anche a questo strumento statistico, in quanto permetterebbe di ampliare la quantità informazioni ottenute da una singola meta-analisi e di promuovere una maggiore trasparenza circa i processi decisionali e il loro relativo impatto sui risultati finali.

Infine, l'applicazione del metodo PIMA alle *Multiverse Meta-Analyses* (PIMMA) rappresenta il prossimo importante passaggio applicativo di tale filosofia di ricerca. Grazie all'applicazione della PIMA alle *Multiverse Meta-Analyses*, infatti, sarà possibile raggiungere conclusioni trasparenti e molto più infomative rispetto a quanto sarebbe possibile fare con un unico studio o un'unica meta-analisi. La PIMMA permetterebbe di individuare quali combinazioni di scelte meta-analitiche influenzano maggiormente i risultati finali e, soprattutto, di condurre inferenze e selezionare le meta-analisi più significative. Ciò rappresenterebbe un'importante svolta nell'ambito della ricerca psicologica poiché consentirebbe, ad esempio, di informare eventuali *policy makers* con una trasparenza e una precisione fino ad ora non ancora raggiunte.

L'applicazione della PIMMA è l'oggetto di questa tesi e, insieme alla *Multiverse Meta-Analysis*, sarà approfondita maggiormente nel prossimo capitolo.

```{=latex}
\newpage
\null
\newpage
```



