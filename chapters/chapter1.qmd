# Crisi di credibilità in psicologia {#sec-Cap.1}

## Introduzione

Nel seguente capitolo verrà presentata la crisi di credibilità che ha coinvolto la ricerca in ambito psicologico negli ultimi decenni. Saranno dunque discusse le diverse componenti di tale crisi, a partire dalla crisi teorica, per poi proseguire con la crisi di validità e con quella di replicazione. Successivamente, verranno esposte le principali cause della crisi di credibilità in psicologia e saranno introdotte alcune delle soluzioni proposte per porvi rimedio. Infine, si illustreranno gli obiettivi di questa tesi.

## Crisi di credibilità

La psicologia sta attraversando una *crisi di credibilità* [@schiavone2023; @malich2022; @morawski2019; @wiggins2019; @gall2017; @vazire2022; @john2012]. La scienza può essere considerata come *"il perseguimento della conoscenza e della comprensione del mondo naturale e sociale derivante da una rigorosa metodologia basata sulle evidenze"* [@the_science_council_our_2024] e si fonda sui principi di riproducibilità, replicabilità [@errington2021; @nosek2022; @opensciencecollaboration2015], misurabilità [@flake2020], trasparenza e accessibilità [@nosek2015; @nosek2022; @errington2021]. Nell'ultimo decennio, però, è emerso che la ricerca in ambito psicologico ha spesso violato tali principi [@opensciencecollaboration2015; @simmons2011; @john2012; @flake2020].

La difficoltà nel riprodurre risultati coerenti con quelli degli studi originali [@opensciencecollaboration2015] e l'utilizzo di procedure discutibili da parte dei ricercatori durante il processo di ricerca [@john2012; @simmons2011; @flake2020] sono alcuni dei principali fattori che hanno compromesso l'affidabilità dei risultati degli studi psicologici.

La crisi di credibilità, però, non riguarda solamente la psicologia, ma coinvolge anche molte altre aree di ricerca, come la biologia e le altre scienze sociali [@errington2021; @fletcher2021; @ioannidis2005]. Quest'ampia diffusione della crisi di credibilità ha di conseguenza minato la fiducia che le persone ripongono nella scienza in generale [@korbmacher2023].

Per quanto riguarda la psicologia nello specifico, la crisi di credibilità è associata strettamente alla *crisi di replicazione*, cioè alla difficoltà nel riprodurre risultati coerenti rispetto a quelli degli studi originali [@malich_introduction_2022; @hutmacher2024]. Tale difficoltà è dovuta all'ampia diffusione di falsi positivi all'interno della letteratura scientifica [@head2015; @errington2021; @scheel2022].

La crisi di replicazione è emersa durante il decennio scorso, a partire dagli articoli di @simmons2011 e, soprattutto, dai risultati dello studio del progetto @opensciencecollaboration2015. @simmons2011 hanno sostenuto che la discrezionalità - da loro denominata "*gradi di libertà*" - del ricercatore nell'utilizzo di procedure di codifica e analisi dei dati inflaziona la proporzione dei falsi positivi [@simmons2011]. Lo studio di @opensciencecollaboration2015, poi, ha evidenziato che i risultati degli studi in ambito psicologico sono difficilmente replicabili [@opensciencecollaboration2015].

Alla base della crisi di replicazione in psicologia, però, risiedono due ulteriori "crisi": la crisi di validità e la crisi teorica. La *crisi di validità* consiste nella scarsa validità interna, esterna, statistica e di costrutto che spesso caratterizza i disegni di ricerca in ambito psicologico [@flake2020; @schimmack2021; @vazire2022]. La *crisi teorica*, invece, riguarda la vaghezza e la mancanza di formalizzazione delle teorie psicologiche, la cui falsificazione risulta quindi spesso complicata o addirittura impossibile [@oberauer2019; @eronen2021; @scheel_why_2021; @scheel2022]. Nonostante tali limiti nell'ambito della ricerca psicologica siano emersi prepotentemente solo nell'ultimo decennio, essi in realtà sono noti fin dalla metà del secolo scorso [@cronbach1955; @meehl1967; @meehl1978].

Le cause della crisi di credibilità sono molteplici. Diversi autori, però, ritengono che le cause principali riguardino le politiche di pubblicazione degli articoli scientifici e la competizione all'interno del mondo accademico [@fanelli2010; @bakker2012; @tiokhin2019; @callard2022; @smaldino2016; @john2012; @head2015]. Le riviste scientifiche, infatti, privilegiano la pubblicazione di studi innovativi e che abbiano ottenuto dei risultati positivi (*publication bias*) [@bakker2012; @tiokhin2019; @head2015; @nosek2022]. Al tempo stesso tendono ad ignorare gli studi di replicazione [@simmons2011; @schmidt2009], cioè quegli studi che mirano a riprodurre dei risultati simili a quelli delle ricerche originali ripetendo la procedura sperimentale di quest'ultime. La carriera di un ricercatore, inoltre, dipende fortemente dalla quantità di articoli pubblicati [@smaldino2016; @head2015; @fanelli2010]. Questa combinazione incentiva i ricercatori ad abusare di tutte quelle procedure che permettono loro di aumentare le possibilità di ottenere dei risultati che confermino le proprie ipotesi [@morawski2019; @callard2022; @simmons2011] - ipotesi molte volte sostenute da scarse basi teoriche [@oberauer2019; @eronen2021; @scheel2022; @hutmacher2024]. Ciò contribuisce all'ampia diffusione di falsi positivi all'interno della letteratura [@simmons2011; @scheel2022; @head2015; @errington2021; @bakker2012].

Nella letteratura scientifica psicologica, infatti, prevalgono ricerche apparentemente valide e innovative, le quali però sono in realtà per la maggior parte caratterizzate da una scarsa potenza statistica [@smaldino2016; @schimmack2021; @nosek2022], da una fragile teoria di riferimento [@oberauer2019; @eronen2021] e da pratiche di ricerca e di misurazione discutibili [@john2012; @simmons2011; @kerr1998; @head2015; @flake2020].

Negli ultimi anni sono state avanzate diverse soluzioni per sopperire ai diversi tipi di crisi che affligono la ricerca in ambito psicologico. Ad oggi, quelle più diffuse sono la pre-registrazione delle ricerche [@nosek2015; @lakens2019], i *Registered Reports* [@nosek2014; @scheel2021; @soderberg2021] e le pratiche di *Open Science* [@nosek2015; @nosek2012; @hagger2022]. Infine, come verrà esposto più avanti, la *Multiverse Analysis* [@steegen2016] e la *Post-Selection Inference in Multiverse Analysis* [@girardi2024] rappresentano alcuni dei possibili e più recenti rimedi sviluppati per affrontare la crisi di replicazione in psicologia.

Nei paragrafi successivi verranno affrontate le diverse crisi che contribuiscono alla crisi di credibilità in psicologia. Le crisi verranno discusse nel seguente ordine: crisi teorica, crisi di validità e crisi di replicazione, nonostante siano emerse in un ordine cronologicamente invertito (ie, dapprima è emersa la crisi di replicazione [@ioannidis2005], poi la crisi di validità [@schimmack2021] e solo infine quella teorica [@eronen2021]). La scelta di presentarle in tal modo risiede nel fatto che, al fine di poter replicare con successo uno studio, è necessario che le variabili che esso indaga siano misurate correttamente (crisi di validità) e, prima ancora, che le ipotesi valutate siano dedotte da teorie solide e ben specificate (crisi teorica).

Successivamente verranno affrontate le cause della crisi di replicazione e alcune possibili soluzioni. Infine, saranno presentati gli obiettivi di questa tesi.

## Crisi teorica

La crisi teorica è uno degli elementi alla base della crisi di replicazione in ambito psicologico [@oberauer2019; @scheel_why_2021; @eronen2021]. La crisi teorica riguarda l'incoerenza logica tra le teorie psicologiche e le ipotesi dedotte da esse, oltre che la conseguente invalidità dei test empirici utilizzati per valutarle [@oberauer2019]; tale incoerenza tra le teorie, le relative ipotesi e i test rende difficile, e spesso impossibile, la falsificazione stessa delle teorie psicologiche [@oberauer2019; @eronen2021; @scheel_why_2021]. Inoltre, la vaghezza delle teorie e l'arbitrarietà nello sviluppo di ipotesi e valutazioni empiriche contribuiscono all'incontrollato aumento dei "*gradi di libertà*" del ricercatore e all'inflazione dell'errore del I tipo [@oberauer2019]; ciò provoca una sproporzione di falsi positivi, che rappresenta la causa primaria della crisi di replicazione [@simmons2011; @opensciencecollaboration2015; @head2015; @errington2021; @scheel2022].

Altri autori, però, sostengono al contrario che la crisi teorica non sia una delle cause della crisi di replicazione in psicologia [@trafimow2016]. Secondo @trafimow2016, infatti, non è necessaria una teoria ben specificata per poter replicare efficacemente i risultati di uno studio, ma anzi è addirittura possibile replicare con successo una ricerca anche in assenza di una teoria di riferimento. Questo poiché la replicazione valuta solo le ipotesi formulate a partire da una teoria, ma non la teoria stessa; di conseguenza, il fallimento nella verifica di un'ipotesi falsifica solamente l'ipotesi e non per forza la teoria da cui essa è stata dedotta.

In questo modo, però, le conclusioni di @trafimow2016 negano la possibilità di sviluppare valutazioni empiriche in grado di falsificare le teorie psicologiche, il che è alla base dello sviluppo della conoscenza scientifica [@scheel_why_2021].

Le affermazioni di @trafimow2016, inoltre, corroborano la tesi di @oberauer2019, secondo la quale in psicologia viene spesso fatta confusione tra la ricerca orientata alla scoperta (*discovery-oriented*) e quella finalizzata alla valutazione delle teorie (*theory-testing)*. La differenza tra i due tipi di ricerca scientifica riguarda il grado con cui la falsificazione di un'ipotesi mina la credibilità della teoria da cui essa è stata dedotta. Se la confutazione di un’ipotesi, infatti, comporta l'invalidazione della teoria originale si parla di ricerca finalizzata alla valutazione della teoria (*theory-testing*); al contrario, se la falsificazione di un'ipotesi non ha alcun impatto sulla teoria originale, si tratta di ricerca *discovery-oriented* [@oberauer2019].

Sempre secondo @oberauer2019, è proprio tale confusione tra i due metodi di ricerca che induce gli psicologi ad applicare alla ricerca *discovery-oriented* le procedure sviluppate per la ricerca finalizzata alla valutazione delle teorie. Questo utilizzo incongruente di determinati strumenti statistici (come le soglie dell'errore del I e del II tipo) contribuisce alla proliferazione dei falsi positivi presenti in letteratura e, quindi, alla crisi di replicazione [@oberauer2019].

Altre cause della crisi teorica in psicologia sembrano essere però intrinseche alla scienza psicologica stessa [@eronen2021]. Secondo @eronen2021, infatti, molte aree della ricerca psicologica non dispongono di sufficienti evidenze empiriche a supporto di fenomeni chiari e ben definiti. Di conseguenza, l'assenza di fenomeni psicologici precisi consente ai ricercatori di sviluppare un numero indefinito di teorie, senza la possibilità di distinguere sulla base di evidenze empiriche quali siano vere e quali false [@eronen2021].

La carenza di fenomeni sufficientemente supportati dalle evidenze è dovuta anche alla scarsa validità dei costrutti psicologici e degli strumenti utilizzati per la loro misurazione [@eronen2021]. Per @eronen2021, ciò costituisce un ulteriore elemento della crisi teorica in psicologia.

Infine, la crisi teorica in ambito psicologico è dovuta anche alla difficoltà nell'inferire e valutare le relazioni causali tra le variabili psicologiche [@eronen2021]. Questo in quanto esse non sono direttamente osservabili e sono strettamente interdipendenti tra loro; di conseguenza, risulta complesso sviluppare disegni sperimentali nei quali gli interventi dello sperimentatore riescano a manipolare solamente la variabile indipendente, senza modificare i valori di altre variabili psicologiche. Tale difficoltà riduce di fatto la possibilità di stabilire delle relazioni causali tra le variabili psicologiche prese in considerazione [@eronen2021].

Ad oggi, la maggior parte delle soluzioni proposte per risolvere la crisi di replicazione in psicologia si è concentrata sulle procedure di codifica, analisi e presentazione dei dati [@scheel_why_2021; @oberauer2019; @nosek2014; @nosek2015; @lakens2019; @scheel2022]. Questo tentativo di agire direttamente sulla fase empirica di conferma o confutazione delle ipotesi rappresenta però "più uno sforzo di curare il sintomo, piuttosto che di eradicare la causa" della scarsa replicabilità degli studi psicologici [@oberauer2019, p. 1608].

Il problema alla base, infatti, riguarda la scarsa specificazione e coerenza tra le teorie psicologiche, le ipotesi e le procedure empiriche sviluppate per valutarle [@oberauer2019]. Secondo @oberauer2019, ciò spesso impedisce ai ricercatori di poter anche solo confermare o confutare le proprie teorie, di fatto ostacolando il processo scientifico di accumulazione della conoscenza, il quale avviene proprio attraverso la falsificazione delle teorie stesse. Una possibile soluzione, dunque, consiste nel formalizzare matematicamente le proprie teorie, così da esplicitare il processo di derivazione delle ipotesi e favorire una loro eventuale falsificazione [@oberauer2019; @scheel2022].

Secondo @eronen2021, però, tale soluzione non risolverebbe i problemi alla radice della crisi teorica. Una più precisa formalizzazione delle teorie psicologiche, infatti, non comporterebbe per forza una maggiore definizione dei fenomeni psicologici, non aumenterebbe la validità dei costrutti e degli strumenti di misurazione utilizzati e, infine, non semplificherebbe la possibilità di stabilire relazioni causali tra le variabili psicologiche indagate.

Secondo alcuni autori, poi, la psicologia è una scienza ancora troppo immatura per la formalizzazione matematica e la valutazione di ipotesi ben definite [@scheel_why_2021; @fife2022; @scheel2022].

In sintesi, dunque, la crisi teorica in psicologia può essere affrontata favorendo le ricerche esplorative finalizzate a definire più chiaramente i fenomeni psicologici, promuovendo una maggiore precisione nella definizione dei costrutti psicologici e sviluppando misure valide [@eronen2021; @scheel_why_2021; @scheel2022]. Una volta terminato tale processo, i ricercatori possiederanno sufficienti informazioni e strumenti per formalizzare e valutare più precisamente le proprie teorie e le relative ipotesi, come proposto da @oberauer2019.

## Crisi di validità

L'affidabilità dei risultati delle ricerche in ambito psicologico è compromessa anche dalla recente crisi di validità [@flake2020; @schimmack2021; @vazire2022]. Come già accennato sopra, l'incerta validità dei costrutti psicologici e degli strumenti utilizzati per misurare le variabili psicologiche mina la credibilità della scienza psicologica [@flake2020; @schimmack2021; @vazire2022; @eronen2021; @scheel_why_2021].

L'assenza di costrutti e di misure valide, infatti, è una delle cause della crisi di replicazione in psicologia [@flake2020; @schimmack2021; @vazire2022]. Questo poiché, anche nel caso in cui un risultato venisse replicato con successo, esso potrebbe comunque non essere valido [@errington2021]. Ad esempio, un ricercatore potrebbe credere di star misurando il quoziente intellettivo (QI) individuale, quando in realtà ciò che sta misurando sono le funzioni esecutive del soggetto; ciononostante, il test che utilizza per misurare tale variabile latente (il QI) produce sempre lo stesso risultato, anche se sta misurando la variabile errata. In questo caso il test è altamente attendibile (riproduce sempre lo stesso risultato), ma non è valido, cioè non misura ciò che si prefigge di misurare.

Una chiara definizione dei costrutti e l'appropriatezza degli strumenti di misurazione utilizzati per valutarli sono alla base della costruzione delle teorie scientifiche e dei disegni sperimentali, e quindi dell'accumulazione della conoscenza scientifica [@flake2020; @eronen2021; @scheel_why_2021]. Nonostante ciò, gran parte degli articoli pubblicati nella letteratura delle scienze sociali non riporta alcun riferimento alla validità degli strumenti di misurazione utilizzati [@flake2020].

La validità, nello specifico, si scompone in quattro elementi: la validità interna, la validità esterna, la validità di costrutto e la validità statistica.

La *validità interna* riguarda "il livello con il quale il disegno sperimentale è in grado di sostenere le inferenze causali tra le variabili indagate" [@flake2020, p. 548]. La *validità esterna* si riferisce, invece, alla generalizzabilità dei risultati di una ricerca [@flake2020; @vazire2022]. La *validità di costrutto* concerne il grado con cui una "variazione quantitativa di una variabile riflette la variazione quantitativa del costrutto che deve misurare" [@schimmack2021, p. 1]. Infine, la *validità statistica* riguarda la coerenza delle conclusioni rispetto all'analisi statistica realizzata [@flake2020].

Ad oggi nell'ambito della ricerca psicologica, tutte e quattro queste componenti della validità sono spesso ignorate [@flake2020; @vazire2022; @schimmack2021]. Per quanto riguarda la validità interna, ad esempio, solamente una scarsa proporzione di studi psicologici presenta dei modelli che esplicitano le predizioni relative alle relazioni causali tra le variabili indagate [@vazire2022].

La validità esterna, invece, è compromessa ad esempio dal fatto che solitamente i campioni utilizzati nelle ricerche in ambito psicologico sono estratti da un'unica popolazione: gli studenti universitari dei corsi di psicologia [@vazire2022]. Il fatto di utilizzare campioni di soggetti provenienti principalmente da società WEIRD, cioè occidentali (*Western*), con elevati livelli di istruzione (*Educated*), industrializzate (*Industrialized*), ricche (*Rich*) e democratiche (*Democratic*), ostacola la possibilità di poter generalizzare i risultati su altri tipi di popolazione [@vazire2022].

Il problema della validità statistica è stato quello maggiormente affrontato nel recente movimento che ha messo al centro il miglioramento della replicabilità degli studi in ambito psicologico [@vazire2022; @schimmack2021; @flake2020; @scheel2022]. La scarsa validità statistica degli studi psicologici, infatti, riguarda proprio tutte quelle pratiche, come ad esempio il *p-hacking* [@head2015], l'*HARKing* [@kerr1998] e le *Questionable Research Practices* [@john2012], che inflazionano l'errore del I tipo e che danno origine quindi ad una sproporzione di falsi positivi all'interno della letteratura psicologica.

La validità di costrutto, infine, è la pietra angolare di qualsiasi teoria e strumento di misurazione. Essa, infatti, stabilisce se un determinato strumento misura esattamente ciò che intende misurare [@schimmack2021]. Un'insufficiente validità di costrutto, perciò, impedisce al ricercatore di sapere se sta misurando ciò che vuole veramente valutare [@schimmack2021]. Nella ricerca psicologica, la validità di costrutto di numerosi strumenti di misurazione non è nota [@schimmack2021; @vazire2022; @flake2020]. Di conseguenza, in assenza di misure che valutano con certezza determinati costrutti psicologici, risulta impossibile inferire le relazioni tra le variabili indagate [@vazire2022; @schimmack2021].

Una delle principali cause di questa crisi di validità in ambito psicologico sono le *Questionable Measurement Practices* (QMPs) [@flake2020]. Secondo @flake2020, le QMPs sono "tutte quelle decisioni dei ricercatori che mettono in dubbio la validità delle misure utilizzate e, di conseguenza, la validità delle conclusioni stesse dello studio" (p. 456). Le QMPs riguardano principalmente la mancanza di trasparenza dei ricercatori circa la validità interna, esterna, di costrutto e statistica degli strumenti di misurazione utilizzati [@flake2020]. Nella maggior parte degli studi psicologici, infatti, manca qualsiasi riferimento alla validità dei costrutti e degli strumenti utilizzati per valutarli [@flake2020]. Ciò impedisce alla comunità scientifica di stimare l'affidabilità dei risultati pubblicati e, quindi, di valutare criticamente le conclusioni dello studio. Ciò ostacola di conseguenza il processo di accumulazione della conoscenza, che è alla base del processo scientifico [@flake2020].

Una maggiore trasparenza è una delle prime soluzioni per arginare la crisi di validità che caratterizza la psicologia al giorno d'oggi [@flake2020]. Per garantire un sano ed efficiente processo scientifico di controllo e miglioramento, infatti, è necessario esplicitare tutti i passaggi compiuti durante il processo di ricerca relativi alla costruzione, selezione e utilizzo dei costrutti e degli strumenti di misurazione [@flake2020]. A tal proposito, @flake2020 hanno sviluppato una serie di domande che possono aiutare i ricercatori ad evitare le QMPs durante il processo di selezione, utilizzo e presentazione degli strumenti di misurazione utilizzati nella ricerca; queste domande possono inoltre guidare anche il processo di revisione e pubblicazione delle ricerche psicologiche.

Per quanto riguarda il miglioramento della validità di costrutto, @schimmack2021 propone nello specifico un programma di validazione basato sulle raccomandazioni di @cronbach1955, basato cioè su un approccio multi-metodo e su modelli causali di correlazione tra i costrutti e le misure utilizzate. Ciò permetterebbe sul lungo periodo di ottenere costrutti e misure sempre più definiti e precisi [@schimmack2021], favorendo così una riduzione dell'incertezza e dell'inaffidabilità dei risultati della ricerca psicologica e riducendo la diffusione di falsi positivi.

## Crisi di replicazione

La crisi di replicazione è la componente principale della crisi di credibilità che ha travolto la psicologia nell'ultimo decennio [@machery2020; @nosek2022; @morawski2019; @wiggins2019; @malich_introduction_2022]. @scheel2022 riassume la crisi di replicazione come "la consapevolezza dei ricercatori che una parte considerevole dei risultati pubblicati in letteratura potrebbe essere falsa" \[p. 2\]. In pratica, essa consiste nella bassa proporzione di replicazioni che riescono ad ottenere risultati simili agli studi originali ed è dovuta al fatto che una larga parte dei risultati pubblicati nella letteratura psicologica sono dei falsi positivi [@head2015; @errington2021; @scheel2022; @bakker2012].

Uno dei principi alla base dell'accumulazione della conoscenza scientifica è proprio la replicabilità dei risultati [@nosek2022; @schmidt2009]. Un singolo studio, infatti, non è mai sufficiente a provare l'esistenza di un fenomeno o di un effetto [@opensciencecollaboration2015; @nichols2021; @errington2021]. È solo attraverso la replicazione dei risultati di uno studio che è possibile confermare la correttezza delle conclusioni originali e far quindi avanzare la conoscenza scientifica [@nosek2022; @schmidt2009]. "È la replicabilità di un risultato empirico", infatti, "a renderlo un risultato scientifico" [@nosek2022, p.722; @schmidt2009].

Nel primo importante progetto di replicazione di studi psicologici, però, i risultati degli studi originali sono stati replicati solamente tra il 36% e il 47% delle volte [@opensciencecollaboration2015]. Nello specifico, nello studio di replicazione condotto dal progetto @opensciencecollaboration2015, sono stati replicati 100 studi pubblicati in tre riviste scientifiche psicologiche. Utilizzando la significatività statistica del *p-value* (p \< .05) come metodo di valutazione del successo o meno della replicazione, solo il 36.1% degli studi è stato valutato come replicato con successo. Utilizzando come criterio, invece, la circostanza in cui il valore dell'*effect size* dello studio originale rientrasse all'interno dell'intervallo di fiducia (stabilito al 95%) dell'*effect size* dello studio replicato, la percentuale di studi replicati con successo è salita al 47.4%. L'ampiezza degli *effect size* degli studi replicati, inoltre, è risultata essere la metà di quella degli effetti degli studi originali. Ciò, quindi, in sintesi, ha evidenziato per la prima volta che più della metà degli studi in campo psicologico non è replicabile.

Innanzitutto, però, quando si parla di crisi di replicazione è necessario distinguere tra riproducibilità e replicabilità e tra replicazione diretta e replicazione concettuale.

### Differenza tra riproducibilità e replicabilità

Per *riproducibilità* di uno studio si intende il grado con cui è possibile riprodurre gli stessi risultati utilizzando le stesse analisi statistiche, lo stesso codice e gli stessi dati dello studio originale [@nosek2022]. Uno studio può quindi non risultare riproducibile perché gli autori non hanno reso pubblici i dati, il codice e/o le analisi statistiche originali oppure perché la riproduzione degli stessi risultati fallisce a causa di errori nello studio originale o nella riproduzione [@nosek2022].

Con *replicabilità*, invece, si intende il grado con cui è possibile riprodurre dei risultati simili a quelli di partenza ripetendo la procedura sperimentale originale [@nosek2022; @schmidt2009]. Nello specifico, è possibile distinguere tra replicazione diretta e replicazione concettuale. Si può parlare di *replicazione diretta* quando lo studio di replicazione cerca di riprodurre il più fedelmente possibile lo studio originale [@machery2020; @derksen2022]. La categoria di *replicazione concettuale*, invece, è applicata a quegli studi di replicazione che mirano a confermare le stesse conclusioni teoriche originali introducendo, però, delle variazioni nella procedura sperimentale (come ad es., replicare uno studio su una popolazione diversa o con strumenti di misurazione diversi, ecc.) [@derksen2022; @machery2020].

Questa distinzione tra replicazioni dirette e concettuali ha comportato un'ulteriore spaccatura all'interno della comunità scientifica degli psicologi [@machery2020; @derksen2022]. Una parte, infatti, sostiene la superiorità delle replicazioni dirette in quanto sarebbero l'unico metodo valido per confermare o confutare l'esistenza dei fenomeni psicologici [@machery2020; @derksen2022]. L'altra, invece, privilegia le replicazioni concettuali in quanto le reputa maggiormente adatte a rafforzare le teorie e a valutare la variabilità e la dinamicità dei fenomeni psicologici [@derksen2022]. Per questo motivo, l'incapacità delle replicazioni concettuali di replicare i risultati originali viene spesso attribuita proprio alla scarsa aderenza alle procedure originali; dal lato opposto, invece, i fallimenti delle replicazioni dirette vengono giudicati inutili in quanto non permetterebbero di comprendere le cause alla base della mancata riproduzione dei risultati originali, cioè se la replicazione è fallita per errori commessi nello studio originale o in quello replicato [@machery2020]. In entrambi i casi, tali argomentazioni contribuiscono alla sottovalutazione di tutte quelle evidenze che minano la credibilità dei risultati della ricerca in ambito psicologico.

Per superare tale dicotomia, @machery2020 propone la seguente definizione di replicazione: "un esperimento che effettua un ricampionamento di tutti quegli elementi sperimentali dell'esperimento originale considerabili dei fattori casuali" \[p. 547\]. Per fattori causali si intendono gli elementi che possono essere estratti da una popolazione (ad es., gli individui da una popolazione di persone, lo strumento di misurazione da una popolazione di strumenti di misurazione, ecc.). Dunque, gli elementi sperimentali che possono essere considerati dei fattori casuali e che quindi possono subire manipolazioni sono: le unità statistiche, le variabili indipendenti, le variabili dipendenti e il setting [@machery2020].

@machery2020 sottolinea che le unità statistiche e il setting, ad esempio, vengono quasi sempre considerate come un fattore casuale; infatti, rappresentano quelle variabili che subiscono delle modifiche anche nelle replicazioni dirette. Al contrario, le variabili dipendenti dello studio, cioè le misurazioni utilizzate per valutare i costrutti indagati, sono spesso considerate dei fattori fissi, nonostante siano invece spesso estratte casualmente da una popolazione di strumenti di misurazione. Infine, nella maggior parte dei casi, solamente la variabile indipendente è l'unico fattore fisso di un disegno sperimentale.

Di conseguenza, @machery2020 conclude che una replicazione è considerabile tale solo se riproduce fedelmente l'esperimento originale, tranne per quei fattori considerati casuali; nel momento in cui, invece, viene modificato il livello di un fattore che nell'originale era considerato fisso, non è più possibile parlare di "replicazione", in quanto non si starebbe più indagando l'effetto originale, ma un altro tipo di relazione tra variabili.

### Cause principali della crisi di replicazione

La causa principale della crisi di replicazione è la sproporzione di falsi positivi presenti in letteratura [@head2015; @errington2021; @scheel2022; @bakker2012]. È proprio tale sproporzione ad aumentare significativamente le probabilità di fallimento degli studi di replicazione: se il risultato della ricerca originale è un falso positivo, infatti, una sua replicazione difficilmente otterrà un ulteriore falso positivo [@nosek2022], soprattutto in quanto spesso le replicazioni sono caratterizzate da una maggiore *potenza statistica*, cioè da campioni più adeguati in termini di dimensione campionaria.

La diffusione dei falsi positivi nella letteratura scientifica psicologica è stata provata empiricamente [@fanelli_positive_2010]. Nel suo studio, @fanelli_positive_2010 ha esaminato la proporzione di risultati positivi, intesi come risultati che confermano le ipotesi di partenza dei ricercatori, in diverse aree scientifiche. La sua analisi ha rivelato che il 91.5% delle ricerche in ambito psicologico/psichiatrico confermano le ipotesi di partenza, contro, ad esempio, un 70% delle scienze dello spazio (ie, astronomia, astrofisica, ecc.). La probabilità di un articolo in ambito psicologico di riportare un risultato positivo è risultata essere, quindi, cinque volte maggiore rispetto a quella di una ricerca nell'area delle scienze dello spazio.

Questa maggiore probabilità di ottenere risultati positivi è causata non solo dalla maggiore o minore "purezza" della scienza di riferimento, ma anche dalla competitività e dalle politiche di pubblicazione in ambito accademico [@fanelli2010]. In un altro studio @fanelli2010, infatti, ha sostenuto che una maggiore competitività accademica, che spinge i ricercatori a pubblicare quanti più articoli possibile per sopravvivere all'interno delle istituzioni universitarie, causa una maggiore proporzione di risultati positivi pubblicati sulle riviste scientifiche. Il tasso di risultati positivi esaminato spaziava da un 25% negli ambienti meno competitivi, fino ad arrivare al 100% in quelli più competitivi.

Un grado di successo così elevato nel confermare le proprie ipotesi di partenza prevede due spiegazioni principali: o le ipotesi di partenza sono sempre vere, e quindi predicono fenomeni ovvi e sono di conseguenza futili per far avanzare la conoscenza scientifica, oppure sono influenzati da alcuni bias [@fanelli2010]. Inoltre, per essere vera, la prima spiegazione comporterebbe non solo che le ipotesi di partenza siano sempre vere, ma anche che la potenza statistica di ciascuno studio sia del 100% [@fanelli2010]. I dati, però, supportano il fatto che in ambito psicologico la potenza statistica è da sempre stata molto più bassa [@smaldino2016; @cohen1962].

Di conseguenza, risulta più probabile la seconda spiegazione, cioè che l'esagerata prevalenza di risultati positivi nell'ambito della ricerca psicologica sia causata da fattori esterni, come il *confirmation bias* del ricercatore stesso e, soprattutto, dalle politiche di pubblicazione che prediligono le ricerche innovative e che abbiano ottenuto risultati positivi (*publication bias*). Tali politiche, poi, inducono i ricercatori stessi ad utilizzare tutte quelle procedure in grado di garantirgli una maggiore probabilità di ottenere un risultato positivo, oltre che a scartare i risultati "negativi" - fenomeno indicato da @rosenthal1979 come "*file-drawer effect"* [@fanelli2010; @bakker2012; @tiokhin2019; @callard2022; @smaldino2016].

Alla base della crisi di replicazione risiede l'ampia diffusione all'interno della ricerca psicologica della pratica - fortemente criticata - del *Null Hypothesis Significance Testing* (NHST), la quale consiste nel "calcolare la probabilità (*p*) di trovare un effetto almeno uguale o più estremo di quello osservato, assumendo che l'ipotesi nulla sia vera" [@head2015, p. 2]. Tale procedura ha portato alla diffusione dell'idea secondo la quale i risultati che ottengono un valore arbitrario di *p* inferiore allo 0.05 siano significativi e superiori e, quindi, più meritevoli di essere pubblicati rispetto agli altri [@head2015]. Ciò, di conseguenza, ha rappresentato un incentivo all'abuso di tutte quelle pratiche discutibili che permettono al ricercatore di ottenere un *p-value* \< .05 [@head2015].

Le procedure più diffuse che, inflazionando l'errore del I tipo, facilitano il raggiungimento di un *p-value* significativo e che quindi portano ad un aumento della diffusione dei falsi positivi nella letteratura scientifica psicologica sono il *p-hacking* [@head2015], l'*HARKing* [@kerr1998] e le *Questionable Research Practices* (QRPs) [@john2012].

Il *p-hacking* consiste nel "raccogliere e selezionare dati o condurre analisi statistiche fino al punto in cui un risultato non significativo diventa significativo" [@head2015, p. 1]. Le pratiche tipiche del *p-hacking* consistono, ad esempio, nel continuare a raccogliere dati oppure nel fermare il processo di raccolta prima del previsto nel momento in cui una qualsiasi analisi statistica genera un *p-value* significativo (ie, p \< .05); oppure includere od escludere dall'analisi statistica alcune variabili, fino a raggiungere il risultato sperato [@head2015].

Il *p-hacking* rientra nella categoria delle QRPs e contribuisce alla diffusione dei falsi positivi nella letteratura psicologica [@head2015; @john2012]. Infatti, tale "flessibilità nel processo di raccolta, analisi e comunicazione dei dati aumenta drasticamente l'effettivo tasso di falsi positivi" [@simmons2011, p. 1359]. Attraverso queste procedure, l'errore del I tipo può raggiungere anche il valore del 60%; ciò significa che diventa più probabile rilevare un effetto che non esiste rispetto al non rilevarlo affatto [@simmons2011].

L'ampia diffusione del *p-hacking* nell'ambito della ricerca psicologica è dovuta principalmente al fatto che la carriera di un ricercatore dipende in larga parte dalla quantità, e non dalla qualità, di articoli pubblicati; ciò, unito al fatto che le riviste scientifiche privilegiano la pubblicazione di risultati positivi, incentiva i ricercatori a sfruttare tutte quelle procedure che favoriscono la produzione di risultati in linea con le loro ipotesi di partenza [@simmons2011; @head2015; @john2012].

Per gli stessi motivi, però, i ricercatori tendono spesso anche a formulare le loro ipotesi dopo aver raccolto e analizzato i dati. Questo fenomeno è stato definito "*HARKing*" (*Hypothesizing After the Results are Known*) [@kerr1998]. L'*HARKing* consiste nel "presentare le proprie ipotesi, formulate dopo aver raccolto e analizzato i dati, come se fossero state formulate prima della raccolta e dell'analisi dei dati" [@kerr1998, p. 196].

Come il *p-hacking*, anche questa pratica comporta un aumento del reale valore dell'errore del I tipo, e quindi della probabilità di ottenere dei falsi positivi [@kerr1998]. Il rischio dell'HARKing è, inoltre, quello di "sviluppare intere teorie volte a giustificare dei falsi positivi" [@kerr1998, p. 205]. Anche in questo caso, le politiche di pubblicazione sono uno dei principali incentivi che spinge i ricercatori in ambito psicologico ad abusare di tale procedura [@kerr1998].

Sia la pratica del *p-hacking* che quella dell'*HARKing* rientrano nella più ampia categoria delle *Questionable Research Practices* (QRPs) [@john2012]. Le QRPs sono tutte quelle procedure utilizzate dal ricercatore nella fase di raccolta, selezione, analisi e presentazione dei dati e dei risultati che aumentano le probabilità di ottenere un risultato positivo [@john2012]. Tra queste, rientra ovviamente anche la pratica di presentare un risultato come se esso fosse stato previsto fin dall'inizio (*HARKing*). Oltre alle procedure tipiche del *p-hacking* presentate sopra, altri esempi di QRPs consistono nel non specificare tutte le caratteristiche dello studio, arrotondare il *p-value* affinchè risulti inferiore alla soglia di 0.05 (ad es., con *p-value* = .054) e falsificare direttamente i dati [@john2012].

Nella ricerca condotta da @john2012, volta ad investigare il grado di diffusione di tali pratiche all'interno del campo della ricerca in ambito psicologico, è emerso che più del 90% degli psicologi che hanno partecipato allo studio ha commesso almeno una QRPs nell'arco della sua carriera. @john2012 arrivano quindi a concludere che le QRPs più che un'eccezione, rappresentano la norma all'interno del mondo accademico della psicologia. Ciò, non solo danneggia la credibilità della ricerca in ambito psicologico, ma porta anche i ricercatori stessi a concentrare i propri sforzi nella replicazione di risultati probabilmente falsi e frutto di manipolazioni [@john2012; @simmons2011].

Alcuni autori, però, hanno evidenziato dei limiti nello studio di @john2012, i quali inficerebbero le conclusioni stesse della ricerca [@fiedler2016]. Il questionario proposto da @john2012 al proprio campione di ricercatori in ambito psicologico, infatti, presenterebbe delle lacune circa la sua validità interna ed esterna [@fiedler2016].

Secondo @fiedler2016, diversi quesiti risultano ambigui e spesso sottintendono pratiche perfettamente lecite. Ad esempio, il quesito "non ho riportato tutte le variabili dipendenti di uno studio", può sì rappresentare una QRPs nel senso di aver "intenzionalmente nascosto dei risultati non graditi" [@fiedler2016, p. 46]. Ciononostante, può anche riferirsi al fatto di non aver riportato dei risultati irrilevanti di altre analisi condotte su variabili non pertinenti all'oggetto dello studio, pratica invece considerata legittima [@fiedler2016]. In questo caso, quindi, @fiedler2016 propongono di modificare il quesito in "non ho riportato tutte le variabili dipendenti che sono rilevanti per il risultato ottenuto" \[p. 46\].

L'altra importante criticità dello studio di @john2012 riguarda le conclusioni degli autori circa la prevalenza delle QRPs [@fiedler2016]. @fiedler2016, infatti, evidenziano che nella ricerca di @john2012 è stata indagata la proporzione di ricercatori che "almeno una volta nella vita" hanno commesso delle QRPs, ma non la frequenza con cui ne hanno abusato. Risulta errato e fuorviante, dunque, inferire la frequenza di tali pratiche a partire dalla proporzione di ricercatori che hanno ammesso di essersene serviti almeno una volta [@fiedler2016].

Per questo motivo, @fiedler2016 hanno replicato lo studio originale di @john2012, rendendo meno ambigui determinati quesiti e misurando anche il tasso di frequenza delle QRPs. I risultati rivelano, innanzitutto, una minore proporzione di ricercatori che ammettono di aver commesso delle QPRs almeno una volta (per diversi *item* il tasso equivale alla metà di quello registrato da John et al., 2012). Inoltre, la prevalenza delle QRPs, calcolata a partire dalla proporzione di ammissioni e dalla frequenza con cui i ricercatori hanno commesso determinate QRPs, risulta essere di molto inferiore - addirittura di un ordine di grandezza - rispetto alla proporzione delle ammissioni di aver commesso una QRPs almeno una volta nella vita.

Stando a questi dati, quindi, sembra che il reale tasso di diffusione delle QRPs sia nettamente inferiore rispetto a quello riportato dallo studio di @john2012.

Ciononostante, la sorprendente prevalenza di risultati positivi all'interno della letteratura in ambito psicologico permane [@scheel2021]. In una recente rassegna della letteratura psicologica condotta da @scheel2021, infatti, i risultati "positivi", cioè gli studi che riportano risultati in linea con le ipotesi di partenza, pubblicati secondo gli standard "classici" ammontano al 96% del totale. Il tasso di risultati positivi, invece, di un campione di *Registered Reports* (RRs) è risultato essere "solo" del 44%. Tale discrepanza tra pubblicazioni "classiche" e RRs è dovuta molto probabilmente alla minore influenza nei RRs del *publication bias* e ad una minore prevalenza di QRPs, che, insieme, riducono la tipica inflazione dell'errore del I tipo [@scheel2021].

Le pratiche di ricerca discutibili appena presentate sono un fenomeno diffuso nell'ambito della ricerca psicologica e sono spesso indicate come la causa principale della crisi di replicazione [@errington2021; @head2015; @fanelli2010]. In un ambiente di ricerca sano ed efficiente, però, tali pratiche non troverebbero posto né all'interno delle istituzioni accademiche, né soprattutto nella letteratura scientifica [@nosek2022]. Il fatto che siano così diffuse e che rappresentino quasi la norma all'interno del mondo della ricerca psicologica [@john2012] è dovuto primariamente alle politiche di pubblicazione e alle competizione all'interno del mondo accademico stesso [@john2012; @head2015; @fanelli2010].

## Cause strutturali della crisi di credibilità

Le riviste scientifiche psicologiche, così come i revisori, privilegiano la pubblicazione di ricerche innovative e che abbiano ottenuto risultati positivi [@nosek2022; @head2015; @bakker2012]. La carriera di un ricercatore, poi, è influenzata principalmente dalla quantità di articoli pubblicati in riviste scientifiche prestigiose [@nosek2022; @head2015; @john2012]. Questi due fattori costituiscono la causa primaria dell'esagerata prevalenza di risultati positivi all'interno della letteratura scientifica psicologica [@nosek2022; @head2015; @john2012; @fanelli2010].

I ricercatori, infatti, per poter avanzare nella propria carriera accademica, ma anche solamente per sopravvivere all'interno del mondo della ricerca, devono pubblicare il maggior numero di articoli possibile [@fanelli2010; @john2012; @head2015]. Il fatto che le riviste tendano a selezionare le ricerche innovative e che abbiano ottenuto risultati in linea con le ipotesi di partenza, il cosiddetto *publication bias*, incentiva i ricercatori ad utilizzare tutte quelle procedure di ricerca discutibili (vedi sopra) in grado di garantire al proprio studio una maggiore probabilità di successo [@head2015; @fanelli2010]. Ciò rappresenta la vera causa dell'inflazione dell'errore del I tipo all'interno del mondo della ricerca psicologica e, quindi, della scarsa replicabilità degli studi psicologici [@errington2021].

Le riviste scientifiche tendono a selezionare e a pubblicare le ricerche innovative e che hanno ottenuto risultati positivi principalmente perché sono quest'ultime a ricevere un maggior numero di citazioni [@fanelli2010]. Ciò, di conseguenza, aumenta l'*impact factor* della rivista, cioè il numero di citazioni ricevute dalla rivista stessa, e quindi il suo prestigio [@fanelli2010; @smaldino2016].

A propria volta, i ricercatori sono incentivati a pubblicare nelle riviste più prestigiose, in quanto le possibilità di lavoro e di ricevere fondi sono influenzate dal numero di pubblicazioni in riviste prestigiose [@fanelli2010; @john2012; @head2015]. Questo provoca un circolo vizioso in cui i ricercatori abusano delle QRPs al fine di ottenere risultati positivi così da aumentare le proprie possibilità di pubblicazione all'interno di una rivista prestigiosa [@john2012]

La strategia ottimale, infatti, per massimizzare la possibilità di ottenere un risultato positivo (ie, p \< .05) e quindi di ottenere una pubblicazione, consiste nel condurre numerosi studi con campioni poco numerosi e quindi con una scarsa potenza statistica [@bakker2012]. Una tale strategia può generare un'inflazione dell'errore del I tipo fino al 40% [@bakker2012].

Per raggiungere i livelli di sproporzione di falsi positivi presenti al giorno d'oggi nella letteratura psicologica, però, non è necessario che i ricercatori utilizzino tali strategie coscientemente [@smaldino2016].

@smaldino2016 propongono, infatti, il concetto di "*selezione naturale della scienza scadente*", il quale prevede che "i metodi associati ad un maggiore successo nelle carriere accademiche tenderanno, a parità di condizioni, a diffondersi" \[p. 2\]. Tale fenomeno non prevede alcuna volontarietà nell'utilizzo di QRPs da parte dei ricercatori. Sono, anzi, gli incentivi presenti nel mondo accademico e nell'ambito della pubblicazione che, privilegiando la quantità rispetto alla qualità delle ricerche, "portano ad una selezione naturale di metodi discutibili e a tassi di falsi positivi sempre più elevati" \[p. 13\].

I metodi di ricerca discutibili che garantiscono una maggiore probabilità di ottenere risultati positivi tendono a diffondersi spontaneamente attraverso, ad esempio, i processi di emulazione dei laboratori e dei ricercatori di maggiore successo da parte degli studenti di tali laboratori e da parte dei laboratori e dei team di ricerca "concorrenti" [@smaldino2016].

@smaldino2016 hanno valutato le loro ipotesi attraverso la simulazione di un modello di dinamica delle popolazioni di tipo evoluzionistico in cui "i ricercatori competono per il prestigio e per posti di lavoro in cui la misura del successo è il numero di pubblicazioni e in cui i laboratori più produttivi avranno più "discendenti" che erediteranno i loro metodi" \[p. 6\]. I risultati ottenuti supportano l'ipotesi che un ambiente come quello della ricerca, in cui vengono privilegiati i risultati innovativi e positivi, mentre le replicazioni e i risultati negativi vengono scartati, e in cui è premiata la quantità delle pubblicazioni, "porta inevitabilmente al deterioramento delle pratiche scientifiche" \[p. 13\].

Un altro elemento che contribuisce alla riduzione della qualità della ricerca in ambito psicologico è la competizione tra ricercatori [@tiokhin2019; @fanelli2010; @john2012; @fanelli2010]. La proporzione di articoli rifiutati nel campo delle scienze sociali e comportamentali, infatti, può raggiungere fino al 70-90% [@nosek2012]. Nello specifico, si stima che circa il 50% degli studi psicologici non venga mai pubblicato [@bakker2012].

@tiokhin2019 hanno valutato il grado con cui la competizione può influenzare la precisione nella raccolta delle informazioni e, quindi, la correttezza delle relative conclusioni. Nel loro esperimento, hanno chiesto a dei soggetti sperimentali di indovinare alcune condizioni sperimentali (ad es., "la popolazione è formata da più quadrati gialli piuttosto che blu" o viceversa) ponendoli di fronte ad un compromesso tra la rapidità e l'accuratezza delle proprie conclusioni. I soggetti, infatti, posti di fronte ad uno schermo con ad esempio 25 tessere rovesciate, potevano "scoprire" il colore di ciascuna di esse, con l'obiettivo di indovinare la corretta proporzione tra tessere gialle e tessere blu.

I risultati ottenuti da @tiokhin2019 supportano l'ipotesi che la competizione riduce l'accuratezza delle conclusioni, in quanto riduce la quantità di informazioni raccolte per inferire le conclusioni stesse; inoltre, la competizione non comporta alcun aumento dell'impegno individuale. Tali risultati supportano quindi l'ipotesi che "premiare la priorità di pubblicazione può incentivare gli individui ad acquisire meno informazioni, portando di conseguenza ad una ricerca di qualità inferiore" \[p.1\].

La competizione per lo spazio all'interno delle riviste scientifiche e, quindi, per la possibilità stessa di continuare a condurre il proprio lavoro in quanto ricercatori, è esacerbata inoltre anche dalla crescente precarizzazione di queste figure professionali [@callard2022; @gjorgjioska2019; @fanelli2010].

Infine, la diffusione di pratiche e metodi discutibili nell'ambito della ricerca psicologica è dovuta anche alla crisi teorica che caratterizza quest'area scientifica [@nosek2022; @smaldino2016]. Risulta più semplice, infatti, manipolare il processo di selezione, analisi e presentazione dei dati e dei risultati "in quei campi di ricerca che si distinguono per una minore precisione e chiarezza dei costrutti e delle teorie" [@nosek2022, p.732]. Inoltre, il fatto che le riviste privilegino le ricerche innovative induce i ricercatori a sviluppare sempre nuove teorie, al posto di cercare di confermare o confutare quelle già esistenti - pratica necessaria per favorire l'accumulazione della conoscenza scientifica [@nosek2022; @smaldino2016].

Le molteplici crisi e i numerosi problemi che affliggono la ricerca in ambito psicologico potrebbero far scaturire un senso di rassegnazione ed impotenza circa la possibilità di "riabilitare" la scienza psicologica. Ciononostante, negli ultimi anni sono state avanzate diverse soluzioni in risposta alle numerose evidenze che hanno fatto emergere le crepe strutturali all'interno del mondo della ricerca psicologica.

## Possibili soluzioni alla crisi

Le principali soluzioni proposte negli ultimi anni per cercare di contrastare la crisi di credibilità in psicologia hanno riguardato primariamente la crisi di replicazione [@oberauer2019; @nosek2018; @lakens2019; @nosek2014]. Tali soluzioni mirano a ridurre la diffusione dei falsi positivi nella letteratura attraverso la promozione di una maggiore trasparenza nel processo di raccolta e analisi dei dati e di presentazione dei risultati [@nosek2012; @nosek2015; @nosek2014; @lakens2019]. Tra queste, le proposte che si sono diffuse maggiormente e che si sono rivelate essere più efficaci riguardano la cultura dell'Open Science, come ad esempio la pre-registrazione degli studi e i *Registered Reports* (RRs) [@soderberg2021; @scheel2021; @chambers2022].

La pratica della *pre-registrazione* di uno studio consiste nel registrare le ipotesi e le procedure programmate di analisi dei dati prima di raccogliere i dati stessi [@nosek2018; @lakens2019]. La registrazione delle procedure può essere caricata su un deposito online indipendente, così da permettere a chiunque di poter accedere alle informazioni [@lakens2019]. Tale procedura ha la funzione principale di "permettere ad altri ricercatori di valutare in modo trasparente la capacità di un esperimento di falsificare un'ipotesi" [@lakens2019, p. 222]. Nello specifico, diminuisce l'inflazione dell'errore del I tipo ad esempio impedendo al ricercatore di condurre numerose analisi statistiche per poi riportare solo quelle che hanno ottenuto un risultato positivo [@lakens2019]. La maggiore trasparenza garantita dalla pre-registrazione, però, non garantisce di per sé una maggiore qualità dello studio, ma permette almeno di valutarne la capacità di falsificare le ipotesi di partenza [@lakens2019].

I RRs, invece, "sono una forma di pubblicazione in cui le proposte di studio vengono sottoposte a revisione paritaria e pre-approvate prima che la ricerca venga intrapresa" [@chambers2022, p.29]. Nella pratica, viene chiesto ai ricercatori di specificare le proprie ipotesi, i metodi di raccolta e di selezione dei dati, le procedure di analisi dei dati, ecc., prima di iniziare la ricerca stessa [@chambers2022]. Una volta inviato il piano del disegno sperimentale viene effettuata una prima *peer-review* volta a giudicare la qualità del disegno sperimentale stesso, cioè della coerenza tra ipotesi, test emprici e metodi di analisi dei dati [@chambers2022; @center_for_open_science_registered_2024].

Se il disegno sperimentale viene approvato, la rivista scientifica garantisce al ricercatore la pubblicazione della ricerca, sempre qualora il ricercatore aderisca con precisione al disegno sperimentale pre-approvato [@chambers2022; @center_for_open_science_registered_2024].

Una volta raccolti e analizzati i dati e aver redatto la relazione della ricerca, lo studio viene infine valutato nuovamente per verificare che il ricercatore abbia rispettato tutte le procedure approvate inizialmente [@chambers2022; @center_for_open_science_registered_2024]. In tal caso, la ricerca viene pubblicata, indipendentemente quindi dai risultati ottenuti [@chambers2022].

Tale procedura favorisce, quindi, la riduzione di tutte quelle QRPs che portano ad un'inflazione dell'errore del I tipo e quindi contribuisce a ridurre l'esagerata diffusione di falsi positivi presente nella letteratura scientifica psicologica odierna [@lakens2019].

Alcuni autori, però, sottolineano che i RRs e la pratica della pre-registrazione non risolvono di per sé il problema emerso dalla crisi teorica in psicologia, cioè il fatto che spesso i ricercatori tendono a valutare prematuramente delle teorie ancora poco definite [@lakens2019; @oberauer2019; @scheel2022].

@oberauer2019 riconoscono che la pre-registrazione contribuisce a diminuire i "*gradi di libertà*" del ricercatore e quindi a tenere sotto controllo l'errore del I tipo. Ciononostante, se tale procedura viene utilizzata meccanicamente, non apporta alcun miglioramento al processo che dovrebbe portare il ricercatore a comprendere se le sue teorie od ipotesi siano "pronte" per essere valutate [@oberauer2019; @scheel2022]. Il ricercatore, infatti, anche se pre-registra il suo disegno sperimentale, non è obbligato a sviluppare ipotesi o predizioni fortemente legate alla teoria.

Secondo @oberauer2019, quindi, la pre-registrazione agisce sul sintomo e non sulla causa della crisi di replicazione. La pre-registrazione, nei fatti, risolve unicamente il problema a livello empirico della flessibilità ed arbitrarietà durante il processo di codifica e analisi dei dati. Allo stesso tempo, però, non affronta quello della lassità con cui vengono dedotte innumerevoli ipotesi - anche contrastanti tra loro - da un'unica teoria, in quanto insufficientemente specificata.

Per affrontare a pieno la crisi di replicazione è quindi necessario risolvere il problema dei gradi di libertà del ricercatore sia rispetto al livello empirico di raccolta, analisi e presentazione dei dati, sia rispetto a quello teorico di coerenza e falsificabilità di teorie ed ipotesi [@oberauer2019; @scheel2022; @scheel2022]. A tal fine, sarebbe necessario dunque distinguere nettamente tra le ricerche *discovery-oriented* e quelle finalizzate alla valutazione delle teorie (*theory-testing*) [@oberauer2019; @eronen2021; @head2015]. Tale chiarezza permetterebbe di evitare di applicare alle ricerche *discovery-oriented* i metodi sperimentali e statistici sviluppati per le ricerche di *theory-testing* (come ad es., la soglia dell'errore del I tipo = .05, o dell'errore del II tipo = .20). Ciò diminuirebbe, di conseguenza, l'inflazione dell'errore del I tipo e favorirebbe anche l'utilizzo del metodo di replicazione corretto in base alla tipologia di studio, che per quanto riguarda le ricerche *discovery-oriented* consiste nella replicazione diretta, mentre per le ricerche *theory-testing* si tratterebbe della replicazione concettuale [@oberauer2019].

Relativamente alla crisi di validità, invece, la soluzione principale consiste nell'applicare i principi dell'Open Science, cioè di una maggiore trasparenza di tutte le decisioni prese durante il processo di ricerca circa le variabili indagate e gli strumenti di misurazione utilizzati per valutarle [@flake2020]. @schimmack2021 propone, poi, di favorire tutti quei processi che mirano alla validazione dei costrutti attraverso un approccio multi-metodo basato su modelli causali di relazione tra costrutti e relative variabili.

Infine, come spiegato più sopra, la replicazione dei disegni di ricerca è centrale per l'accumulazione della conoscenza scientifica proprio poiché difficilmente un singolo studio è sufficiente a confermare l'esistenza di un fenomeno o di un effetto [@opensciencecollaboration2015; @nichols2021; @errington2021]. Per tale motivo, la *meta-analisi*, cioè quella procedura statistica finalizzata ad ottenere una sintesi quantitativa dei risultati di diversi studi [@borenstein2009], rappresenta uno dei metodi d'eccellenza per valutare l'accumulazione della conoscenza in specifici ambiti di ricerca. Ciò in quanto sono "le evidenze cumulative o meta-analitiche ottenute da più esperimenti condotti indipendentemente a fornire una base migliore per valutare l'affidabilità dei risultati" [@errington2021, p. 10].

La meta-analisi, inoltre, si può applicare non solo per analizzare i risultati di studi già condotti (ottica retrospettiva), ma anche per analizzare congiuntamente i risultati di ricerche che verranno condotte seguendo uno stesso disegno sperimentale (ottica prospettiva). A tal proposito, le ricerche *multi-lab* rappresentano un recente ambito di applicazione della meta-analisi. Nello specifico, la ricerca *multi-lab* consiste nello svolgimento dello stesso disegno sperimentale condotto in contemporanea in molteplici laboratori da diversi gruppi di ricerca in diversi territori [@lewis2022; @ishii2023]. I dati provenienti dai diversi laboratori sono poi raccolti e analizzati congiuntamente [@lewis2022]. Questa modalità di ricerca permette di ottenere stime più precise degli effetti valutati, in quanto il coinvolgimento di numerosi laboratori consente di aumentare la numerosità campionaria complessiva e quindi la potenza statistica [@lewis2022], cioè la capacità di un disegno sperimentale di rilevare un determinato effetto, qualora esso esista [@cohen1962]. Tale procedura sperimentale consente quindi di affrontare anche la crisi di replicazione più in generale [@lewis2022; @errington2021].

Nell'ambito della ricerca *multi-lab*, e in generale in quello meta-analitico, però, i gradi di libertà dell'insieme dei disegni sperimentali tendono ad aumentare significativamente [@steegen2016]. Una tra le numerose e più recenti soluzioni proposte per risolvere tale limite, oltre che la crisi di replicazione più in generale, è la *Multiverse Analysis* [@steegen2016]. Essa, infatti, permette di valutare in modo trasparente l'impatto che ciascuna decisione analitica effettuata durante il processo di ricerca può avere sull'effetto indagato [@steegen2016]. Inoltre, la *Multivere Analysis*, come verrà esposto nel capitolo X, può essere applicata anche alle meta-analisi.

Il principale limite della *Multiverse Analysis* consiste nell'impossibilità di effettuare inferenze a partire dai risultati di tale metodo statistico [@girardi2024]. Molto recentemente, però, è stata sviluppata una procedura che consente di colmare tale lacuna: la *Post-Selection Inference in Multiverse Analysis* [@girardi2024].

Tali metodi rappresentano l'oggetto del seguente lavoro e saranno perciò discussi più approfonditamente ed applicati nei capitoli successivi.

Da ultimo, è doveroso sottolineare che la crisi di credibilità in psicologia verrà difficilmente superata fintantoché persisteranno tutti quegli incentivi accademici ed economici che favoriscono l'abuso di QRPs e QMPs [@head2015; @fanelli2010; @bakker2012; @smaldino2016; @callard2022]. Sono proprio tali incentivi a rappresentare le cause strutturali della diffusione dei falsi positivi all'interno della letteratura scientifica psicologica [@john2012; @head2015; @fanelli2010]. Perciò, per affrontare in modo corretto la crisi di replicazione, è necessario dapprima riformare a livello istituzionale le politiche di pubblicazione e di avanzamento di carriera dei ricercatori [@smaldino2016; @nosek2012; @gall2017].

## Obiettivi della tesi

L'obiettivo principale della seguente tesi è introdurre un metodo recentemente sviluppato che applica la *Post-Selection Inference in Multiverse Analysis* direttamente alle meta-analisi. Questa nuova procedura permette di affrontare il problema dell'inflazione dell'errore del I tipo e, quindi, la crisi di replicazione più in generale.

Dopo un introduzione teorica verrà presentata anche un'applicazione pratica di tale metodo.
